{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bda1205b-24d3-4665-b56e-531e8a136fa7",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python interpreter will be restarted.\nCollecting azure-eventhub\n  Using cached azure_eventhub-5.11.2-py3-none-any.whl (305 kB)\nCollecting azure-core<2.0.0,>=1.14.0\n  Using cached azure_core-1.27.1-py3-none-any.whl (174 kB)\nCollecting typing-extensions>=4.0.1\n  Using cached typing_extensions-4.6.3-py3-none-any.whl (31 kB)\nRequirement already satisfied: six>=1.11.0 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.14.0->azure-eventhub) (1.16.0)\nRequirement already satisfied: requests>=2.18.4 in /databricks/python3/lib/python3.9/site-packages (from azure-core<2.0.0,>=1.14.0->azure-eventhub) (2.26.0)\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (3.2)\nRequirement already satisfied: charset-normalizer~=2.0.0 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (2.0.4)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (1.26.7)\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.9/site-packages (from requests>=2.18.4->azure-core<2.0.0,>=1.14.0->azure-eventhub) (2021.10.8)\nInstalling collected packages: typing-extensions, azure-core, azure-eventhub\n  Attempting uninstall: typing-extensions\n    Found existing installation: typing-extensions 3.10.0.2\n    Not uninstalling typing-extensions at /databricks/python3/lib/python3.9/site-packages, outside environment /local_disk0/.ephemeral_nfs/envs/pythonEnv-0b7057c5-cffd-43ea-9f05-5dc88ebfb911\n    Can't uninstall 'typing-extensions'. No files were found to uninstall.\nSuccessfully installed azure-core-1.27.1 azure-eventhub-5.11.2 typing-extensions-4.6.3\nPython interpreter will be restarted.\nOut[1]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f21707cdfd0>"
     ]
    }
   ],
   "source": [
    "# Source with default settings\n",
    "%pip install azure-eventhub\n",
    "from azure.eventhub import EventHubProducerClient, EventHubConsumerClient, EventData\n",
    "\n",
    "import pyspark.pandas as ps\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "\n",
    "\n",
    "from pyspark import SparkContext\n",
    "sc =SparkContext.getOrCreate()\n",
    "\n",
    "connectionString = \"Endpoint=sb://streamdata-f9019e-ns.servicebus.windows.net/;SharedAccessKeyName=rule;SharedAccessKey=Zebuc91qznwl91w9y3Ec1ZEM0YXZXlFX9+AEhHtUxe8=;EntityPath=streamdata-f9019e-ns\"\n",
    "ehConf = {}\n",
    "ehConf['eventhubs.connectionString'] = sc._jvm.org.apache.spark.eventhubs.EventHubsUtils.encrypt(connectionString)\n",
    "\n",
    "df = spark \\\n",
    "  .readStream \\\n",
    "  .format(\"eventhubs\") \\\n",
    "  .options(**ehConf) \\\n",
    "  .load()\n",
    "\n",
    "sales_orders_df = df.select(\"body\").withColumn(\"body\", col(\"body\").cast(\"string\"))\n",
    "\n",
    "sales_orders_df.writeStream.format(\"delta\")\\\n",
    "   .outputMode(\"append\")\\\n",
    "   .option(\"checkpointLocation\", \"/tmp/delta/events/_checkpoints/\")\\\n",
    "   .toTable(\"sales_order_raw_stream\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4fe1989b-b14d-45d5-8056-6610433d2dd6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[2]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f60c0f39ca0>"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "sales_orders_schema = StructType(\n",
    "    [\n",
    "        StructField(\n",
    "            \"clicked_items\", ArrayType(ArrayType(StringType(), True), True), True\n",
    "        ),\n",
    "        StructField(\"customer_id\", LongType(), True),\n",
    "        StructField(\"customer_name\", StringType(), True),\n",
    "        StructField(\"number_of_line_items\", LongType(), True),\n",
    "        StructField(\"order_datetime\", StringType(), True),\n",
    "        StructField(\"order_number\", LongType(), True),\n",
    "        StructField(\n",
    "            \"ordered_products\",\n",
    "            ArrayType(\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"id\", StringType(), True),\n",
    "                        StructField(\"qty\", IntegerType(), True),\n",
    "                        StructField(\"curr\", StringType(), True),\n",
    "                        StructField(\"name\", StringType(), True),\n",
    "                        StructField(\"unit\", StringType(), True),\n",
    "                        StructField(\"price\", IntegerType(), True),\n",
    "                        StructField(\n",
    "                            \"promotion_info\",\n",
    "                            StructType(\n",
    "                                [\n",
    "                                    StructField(\"promo_id\", IntegerType(), True),\n",
    "                                    StructField(\"promo_qty\", IntegerType(), True),\n",
    "                                    StructField(\"promo_disc\", DecimalType(3, 2), True),\n",
    "                                    StructField(\"promo_item\", StringType(), True),\n",
    "                                ]\n",
    "                            ),\n",
    "                            True,\n",
    "                        ),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            True,\n",
    "        ),\n",
    "        StructField(\n",
    "            \"promo_info\",\n",
    "            ArrayType(\n",
    "                StructType(\n",
    "                    [\n",
    "                        StructField(\"promo_id\", IntegerType(), True),\n",
    "                        StructField(\"promo_qty\", IntegerType(), True),\n",
    "                        StructField(\"promo_disc\", DecimalType(3, 2), True),\n",
    "                        StructField(\"promo_item\", StringType(), True),\n",
    "                    ]\n",
    "                ),\n",
    "                True,\n",
    "            ),\n",
    "            True,\n",
    "        ),\n",
    "    ]\n",
    "  )\n",
    "\n",
    "df1 = (spark.readStream.format(\"delta\").table(\"sales_order_raw_stream\")\n",
    "       .select(\"*\")\n",
    "       .withColumn(\"body\", regexp_replace(\"body\", '\"\\\\[', \"[\"))\n",
    "       .withColumn(\"body\", regexp_replace(\"body\", '\\\\]\"', \"]\"))\n",
    "       .withColumn(\"body\", regexp_replace(\"body\",\"\\\\\\\\\", \"\"))\n",
    "       .withColumn(\"body\", expr(\"substring(body, 2, length(body))\"))\n",
    "       .withColumn(\"body\", expr(\"substring(body, 1, length(body)-1)\"))\n",
    "       .select(from_json(col(\"body\"), sales_orders_schema).alias(\"row\"))\n",
    "       .select(\"row.*\")\n",
    "       .withColumn(\"ordered_products\", explode(\"ordered_products\"))\n",
    "       .withColumn(\"order_datetime\", from_unixtime(\"order_datetime\"))\n",
    "       .withColumn(\"product_id\", col(\"ordered_products\").id)\n",
    "       .withColumn(\"unit_price\", col(\"ordered_products\").price)\n",
    "       .withColumn(\"quantity\", col(\"ordered_products\").qty)\n",
    "       \n",
    "      )\n",
    "df1.writeStream.format(\"delta\")\\\n",
    "   .outputMode(\"append\")\\\n",
    "   .option(\"checkpointLocation\", \"/tmp/delta/eventschkforsilver1/_checkpoints/\")\\\n",
    "   .toTable(\"sales_orders_cleansed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "335a3734-3105-4466-8290-7187f01a2b19",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[4]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f60c0f39df0>"
     ]
    }
   ],
   "source": [
    "s = spark.readStream.format(\"delta\").table(\"sales_orders_cleansed\").alias(\"s\")\n",
    "p = spark.read.format(\"delta\").table(\"dim_products\").alias(\"p\")\n",
    "c = spark.read.format(\"delta\").table(\"dim_customers\").alias(\"c\")\n",
    "\n",
    "df2 = s.join(p, s.product_id == p.product_id, \"inner\").join(c, s.customer_id == c.customer_id, \"inner\").select(\"s.order_number\",\"c.customer_key\",\"p.product_key\",col(\"s.order_datetime\").cast(\"date\").alias(\"order_date\"),\"s.unit_price\",\"s.quantity\",expr(\"s.unit_price * s.quantity\").alias(\"total_price\"),)\n",
    "\n",
    "df2.writeStream.format(\"delta\")\\\n",
    "    .outputMode(\"append\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/delta/eventscheckforgoldretailorg1/_checkpoints/\")\\\n",
    "    .toTable(\"fact_sales_orders1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3108bf4b-786e-490f-b309-5fb446a607c5",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Out[5]: <pyspark.sql.streaming.query.StreamingQuery at 0x7f60c0143f70>"
     ]
    }
   ],
   "source": [
    "s = spark.readStream.format(\"delta\").table(\"sales_orders_cleansed\").alias(\"s\")\n",
    "p = spark.read.format(\"delta\").table(\"dim_products\").alias(\"p\")\n",
    "c = spark.read.format(\"delta\").table(\"dim_customers\").alias(\"c\")\n",
    "\n",
    "df3 = s.join(p, s.product_id == p.product_id, \"inner\").join(c, s.customer_id == c.customer_id, \"inner\").groupBy(\"c.customer_key\", \"p.product_key\").agg(sum(\"quantity\").alias(\"total_quantity\"),sum(expr(\"s.unit_price * s.quantity\")).alias(\"sale_amount\"),)\n",
    "\n",
    "df3.writeStream.format(\"delta\")\\\n",
    "    .outputMode(\"complete\")\\\n",
    "    .option(\"checkpointLocation\", \"/tmp/delta/eventscheckforgoldcompleteretailorg1/_checkpoints/\")\\\n",
    "    .toTable(\"fact_customer_sales\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 4126486681898744,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "bronze_silver_gold_stream",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
